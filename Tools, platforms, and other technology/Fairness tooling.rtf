{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033\deflangfe1033{\fonttbl{\f0\fswiss\fprq2\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\colortbl ;\red0\green0\blue255;\red5\green99\blue193;}
{\*\generator Riched20 10.0.19041}{\*\mmathPr\mnaryLim0\mdispDef1\mwrapIndent1440 }\viewkind4\uc1 
\pard\widctlpar\sa160\sl252\slmult1\b\f0\fs22 AI Fairness 360 (IBM)\par
{\b0{\field{\*\fldinst{HYPERLINK http://aif360.mybluemix.net/ }}{\fldrslt{http://aif360.mybluemix.net/\ul0\cf0}}}}\b0\f0\fs22  \par
{{\field{\*\fldinst{HYPERLINK https://github.com/Trusted-AI/AIF360 }}{\fldrslt{https://github.com/Trusted-AI/AIF360\ul0\cf0}}}}\f0\fs22  \par
Supported bias mitigation algorithms\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Optimized Preprocessing ({{\field{\*\fldinst{HYPERLINK "http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention" }}{\fldrslt{\ul\cf1\cf2\ul Calmon et al., 2017}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Disparate Impact Remover ({{\field{\*\fldinst{HYPERLINK "https://doi.org/10.1145/2783258.2783311" }}{\fldrslt{\ul\cf1\cf2\ul Feldman et al., 2015}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Equalized Odds Postprocessing ({{\field{\*\fldinst{HYPERLINK "https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning" }}{\fldrslt{\ul\cf1\cf2\ul Hardt et al., 2016}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Reweighing ({{\field{\*\fldinst{HYPERLINK "http://doi.org/10.1007/s10115-011-0463-8" }}{\fldrslt{\ul\cf1\cf2\ul Kamiran and Calders, 2012}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}\lang1036 Reject Option Classification ({{\field{\*\fldinst{HYPERLINK "https://doi.org/10.1109/ICDM.2012.45" }}{\fldrslt{\ul\cf1\cf2\ul Kamiran et al., 2012}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}\lang1033 Prejudice Remover Regularizer ({{\field{\*\fldinst{HYPERLINK "https://rd.springer.com/chapter/10.1007/978-3-642-33486-3_3" }}{\fldrslt{\ul\cf1\cf2\ul Kamishima et al., 2012}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Calibrated Equalized Odds Postprocessing ({{\field{\*\fldinst{HYPERLINK "https://papers.nips.cc/paper/7151-on-fairness-and-calibration" }}{\fldrslt{\ul\cf1\cf2\ul Pleiss et al., 2017}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Learning Fair Representations ({{\field{\*\fldinst{HYPERLINK "http://proceedings.mlr.press/v28/zemel13.html" }}{\fldrslt{\ul\cf1\cf2\ul Zemel et al., 2013}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Adversarial Debiasing ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1801.07593" }}{\fldrslt{\ul\cf1\cf2\ul Zhang et al., 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Meta-Algorithm for Fair Classification ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1806.06055" }}{\fldrslt{\ul\cf1\cf2\ul Celis et al.. 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Rich Subgroup Fairness ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1711.05144" }}{\fldrslt{\ul\cf1\cf2\ul Kearns, Neel, Roth, Wu, 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Exponentiated Gradient Reduction ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1803.02453" }}{\fldrslt{\ul\cf1\cf2\ul Agarwal et al., 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Grid Search Reduction ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1803.02453" }}{\fldrslt{\ul\cf1\cf2\ul Agarwal et al., 2018}}}}\f0\fs22 , {{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1905.12843" }}{\fldrslt{\ul\cf1\cf2\ul Agarwal et al., 2019}}}}\f0\fs22 )\par

\pard\widctlpar\sa160\sl252\slmult1 Supported fairness metrics\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Comprehensive set of group fairness metrics derived from selection rates and error rates including rich subgroup fairness\par
{\pntext\f1\'B7\tab}Comprehensive set of sample distortion metrics\par
{\pntext\f1\'B7\tab}Generalized Entropy Index ({{\field{\*\fldinst{HYPERLINK "https://doi.org/10.1145/3219819.3220046" }}{\fldrslt{\ul\cf1\cf2\ul Speicher et al., 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Differential Fairness and Bias Amplification ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/1807.08362" }}{\fldrslt{\ul\cf1\cf2\ul Foulds et al., 2018}}}}\f0\fs22 )\par
{\pntext\f1\'B7\tab}Bias Scan with Multi-Dimensional Subset Scan ({{\field{\*\fldinst{HYPERLINK "https://arxiv.org/abs/1611.08292" }}{\fldrslt{\ul\cf1\cf2\ul Zhang, Neill, 2017}}}}\f0\fs22 )\par

\pard\widctlpar\sa160\sl252\slmult1\b\par
Fairlearn\par
{\b0{\field{\*\fldinst{HYPERLINK https://fairlearn.org/ }}{\fldrslt{https://fairlearn.org/\ul0\cf0}}}}\b0\f0\fs22\par
{{\field{\*\fldinst{HYPERLINK https://github.com/fairlearn/fairlearn }}{\fldrslt{https://github.com/fairlearn/fairlearn\ul0\cf0}}}}\f0\fs22\par
Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems.\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\widctlpar\fi-360\li720\sa160\sl252\slmult1 A Python library for fairness assessment and improvement (fairness metrics, mitigation algorithms, plotting, etc.)\par

\pard\widctlpar\sa160\sl252\slmult1 The Fairlearn tookit can assist in assessing and mitigation unfairness in Machine Learning models. It\rquote s impossible to provide a sufficient overview of fairness in ML in this Quickstart tutorial, so we highly recommend starting with our User Guide. Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model. Additionally, fairness has different definitions in different contexts and it may not be possible to represent it quantitatively at all.\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\widctlpar\fi-360\li720\sa160\sl252\slmult1 Metrics for assessing which groups are negatively impacted by a model, and for comparing multiple models in terms of various fairness and accuracy metrics.\par
{\pntext\f1\'B7\tab}Algorithms for mitigating unfairness in a variety of AI tasks and along a variety of fairness definitions.\par

\pard\widctlpar\sa160\sl252\slmult1\b What if tool (Google People and AI Research)\par
{\b0{\field{\*\fldinst{HYPERLINK https://pair-code.github.io/what-if-tool/ }}{\fldrslt{https://pair-code.github.io/what-if-tool/\ul0\cf0}}}}\b0\f0\fs22\par
{{\field{\*\fldinst{HYPERLINK https://github.com/pair-code/what-if-tool }}{\fldrslt{https://github.com/pair-code/what-if-tool\ul0\cf0}}}}\f0\fs22\par
Visually probe the behavior of trained machine learning models, with minimal coding.\par
A key challenge in developing and deploying responsible Machine Learning (ML) systems is understanding their performance across a wide range of inputs.\par
Using WIT, you can test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data, and for different ML fairness metrics.\par
The What-If Tool can work with any python-accessible model in Notebook environments, and will work with most models hosted by TF-serving in Tensorboard.\par
The What-If Tool supports:\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa160\sl252\slmult1 binary classification*\par
{\pntext\f1\'B7\tab}multi-class classification\par
{\pntext\f1\'B7\tab}regression tasks\par

\pard\widctlpar\sa160\sl252\slmult1 * Fairness optimization strategies are available only with binary classification models due to the nature of the strategies themselves.\par
In the What-If Tool, Counterfactuals are datapoints that are most similar to a selected datapoint, but are classified differently by a model.\par
For binary classification models, counterfactuals are the most similar datapoint to a selected datapoint that is predicted in the opposite class or label by a model.\par
For regression models, counterfactuals are calculated when the difference in prediction score between the selected datapoint and a candidate counterfactual is equal or greater to the \ldblquote counterfactual threshold\rdblquote . The counterfactual threshold default is set to the standard deviation of the prediction values and can be adjusted by the user.\par
For multi-class models, the counterfactual is the most similar datapoint to a selected datapoint, but is classified as any class other than the selected datapoint\rquote s class.\par
\b Sagemaker clarify (Amazon)\par
{\b0{\field{\*\fldinst{HYPERLINK https://aws.amazon.com/sagemaker/clarify/ }}{\fldrslt{https://aws.amazon.com/sagemaker/clarify/\ul0\cf0}}}}\b0\f0\fs22\par
Provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.\par
Biases are imbalances in the training data or prediction behavior of the model across different groups, such as age or income bracket. Biases can result from the data or algorithm used to train your model. For instance, if an ML model is trained primarily on data from middle-aged individuals, it may be less accurate when making predictions involving younger and older people. The field of machine learning provides an opportunity to address biases by detecting them and measuring them in your data and model. You can also look at the importance of model inputs to explain why models make the predictions they do.\par
Amazon SageMaker Clarify detects potential bias during data preparation, after model training, and in your deployed model by examining attributes you specify. For instance, you can check for bias related to age in your initial dataset or in your trained model and receive a detailed report that quantifies different types of possible bias. SageMaker Clarify also includes feature importance graphs that help you explain model predictions and produces reports which can be used to support internal presentations or to identify issues with your model that you can take steps to correct.\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\widctlpar\fi-360\li720\sa160\sl252\slmult1 Identify imbalances in data\par
{\pntext\f1\'B7\tab}Check trained models for bias\par
{\pntext\f1\'B7\tab}Monitor model for bias\par
{\pntext\f1\'B7\tab}Understand model\par
{\pntext\f1\'B7\tab}Monitor model for changes in behavior\par
{\pntext\f1\'B7\tab}explain individual model predictions\par

\pard\widctlpar\sa160\sl252\slmult1\b ML Fairness gym (Google)\par
{\b0{\field{\*\fldinst{HYPERLINK https://github.com/google/ml-fairness-gym }}{\fldrslt{https://github.com/google/ml-fairness-gym\ul0\cf0}}}}\b0\f0\fs22\par
ML-fairness-gym is a set of components for building simple simulations that explore the potential long-run impacts of deploying machine learning-based decision systems in social environments. As the importance of machine learning fairness has become increasingly apparent, recent research has focused on potentially surprising long term behaviors of enforcing measures of fairness that were originally defined in a static setting. Key findings have shown that under specific assumptions in simplified dynamic simulations, long term effects may in fact counteract the desired goals. Achieving a deeper understanding of such long term effects is thus a critical direction for ML fairness research. ML-fairness-gym implements a generalized framework for studying and probing long term fairness effects in carefully constructed simulation scenarios where a learning agent interacts with an environment over time. This work fits into a larger push in the fair machine learning literature to design decision systems that induce fair outcomes in the long run, and to understand how these systems might differ from those designed to enforce fairness on a one-shot basis.\par
Using fairness-gym in your research\par
ML-fairness-gym brings reinforcement learning-style evaluations to fairness in machine learning research. Here is a suggested pattern for using the ML-fairness-gym as part of the research process. Others may be added here as we continue to grow.\par
Evaluating a proposed ML algorithm\par
Here are suggested steps when evaluating a proposed new fair ML algorthm:\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\widctlpar\fi-360\li720\sa160\sl252\slmult1 Choose a simulation environment.\par
{\pntext\f1\'B7\tab}Decide on metrics that you would like to measure for that environment.\par
{\pntext\f1\'B7\tab}Choose baseline agents and choose what reward functions they will optimize.\par
{\pntext\f1\'B7\tab}Write an agent that uses your new algorithm.\par
{\pntext\f1\'B7\tab}Compare metrics between your baseline agents and your fair agent. Some utilities for building experiments are provided in run_util.py. For example, run_simulationis a simple function that runs an experiment and returns metric measurements.\par
{\pntext\f1\'B7\tab}Explore parameter settings in your simulation environment - are there different regimes?\par

\pard\widctlpar\sa160\sl252\slmult1 We provide some implementations of environments, agents, and metrics, but they are by no means comprehensive. Feel free to implement your own and contribute to ML-fairness-gym!\par
\b Aequitas\par
{\b0{\field{\*\fldinst{HYPERLINK https://dssg.github.io/aequitas/ }}{\fldrslt{https://dssg.github.io/aequitas/\ul0\cf0}}}}\b0\f0\fs22  \par
Aequitas is an open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive tools.\par
Aequitas will help you:\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\widctlpar\fi-360\li720\sa160\sl252\slmult1 Understand where biases exist in your model(s)\par
{\pntext\f1\'B7\tab}Compare the level of bias between groups in your sample population (bias disparity)\par
{\pntext\f1\'B7\tab}Visualize absolute bias metrics and their related disparities for rapid comprehension and decision-making\par

\pard\widctlpar\sa160\sl252\slmult1 Our goal is to support informed and equitable action for both machine learnining practitioners and the decision-makers who rely on them.\par
}
 