{\rtf1\ansi\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset238 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 Agarwal, A., Beygelzimer, A., Dud\'edk, M., Langford, J., & Wallach, H. (2018, July). A reductions approach to fair classification. In International Conference on Machine Learning (pp. 60-69). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf }}{\fldrslt{http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf\ul0\cf0}}}}\f0\fs22\par
Agarwal, A., Dud\'edk, M., & Wu, Z. S. (2019, May). Fair regression: Quantitative definitions and reduction-based algorithms. In International Conference on Machine Learning (pp. 120-129). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf }}{\fldrslt{http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf\ul0\cf0}}}}\f0\fs22\par
Albarghouthi, A., D'Antoni, L., Drews, S., & Nori, A. V. (2017). Fairsquare: probabilistic verification of program fairness. Proceedings of the ACM on Programming Languages, 1(OOPSLA), 1-30. {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3133904 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3133904\ul0\cf0}}}}\f0\fs22\par
Barocas, S., & Selbst, A. D. (2016). Big data's disparate impact. Calif. L. Rev., 104, 671. {{\field{\*\fldinst{HYPERLINK http://www.datascienceassn.org/sites/default/files/Big%20Data%27s%20Disparate%20Impact.pdf }}{\fldrslt{http://www.datascienceassn.org/sites/default/files/Big%20Data%27s%20Disparate%20Impact.pdf\ul0\cf0}}}}\f0\fs22\par
Baur, T., Mehlmann, G., Damian, I., Lingenfelser, F., Wagner, J., Lugrin, B., ... & Gebhard, P. (2015). Context-Aware Automated Analysis and Annotation of Social Human--Agent Interactions. ACM Transactions on Interactive Intelligent Systems (TiiS), 5(2), 1-33. {{\field{\*\fldinst{HYPERLINK https://www.researchgate.net/profile/Tobias-Baur-3/publication/279748956_Context-Aware_Automated_Analysis_and_Annotation_of_Social_Human-Agent_Interactions/links/56f3c58808ae95e8b6ccff71/Context-Aware-Automated-Analysis-and-Annotation-of-Social-Human-Agent-Interactions.pdf }}{\fldrslt{https://www.researchgate.net/profile/Tobias-Baur-3/publication/279748956_Context-Aware_Automated_Analysis_and_Annotation_of_Social_Human-Agent_Interactions/links/56f3c58808ae95e8b6ccff71/Context-Aware-Automated-Analysis-and-Annotation-of-Social-Human-Agent-Interactions.pdf\ul0\cf0}}}}\f0\fs22\par
Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29, 4349-4357. {{\field{\*\fldinst{HYPERLINK https://papers.nips.cc/paper/6228-estimating-the-size-of-a-large-network-and-its-communities-from-a-random-sample.pdf }}{\fldrslt{https://papers.nips.cc/paper/6228-estimating-the-size-of-a-large-network-and-its-communities-from-a-random-sample.pdf\ul0\cf0}}}}\f0\fs22\par
Broesch, J., Barrett, H. C., & Henrich, J. (2014). Adaptive content biases in learning about animals across the life course. Human Nature, 25(2), 181-199. {{\field{\*\fldinst{HYPERLINK https://robobees.seas.harvard.edu/files/culture_cognition_coevol_lab/files/broesch_barrett_henrich_2014.pdf }}{\fldrslt{https://robobees.seas.harvard.edu/files/culture_cognition_coevol_lab/files/broesch_barrett_henrich_2014.pdf\ul0\cf0}}}}\f0\fs22\par
Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1608.07187.pdf?ref=hackernoon.com }}{\fldrslt{https://arxiv.org/pdf/1608.07187.pdf?ref=hackernoon.com\ul0\cf0}}}}\f0\fs22\par
Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., & Elhadad, N. (2015, August). Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1721-1730). {{\field{\*\fldinst{HYPERLINK https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.1729&rep=rep1&type=pdf }}{\fldrslt{https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.1729&rep=rep1&type=pdf\ul0\cf0}}}}\f0\fs22\par
Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1703.00056 }}{\fldrslt{https://arxiv.org/pdf/1703.00056\ul0\cf0}}}}\f0\fs22\par
Corbett-Davies, S., & Goel, S. (2018). The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1808.00023.pdf }}{\fldrslt{https://arxiv.org/pdf/1808.00023.pdf\ul0\cf0}}}}\f0\fs22 ,\par
Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., & Huq, A. (2017, August). Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining (pp. 797-806). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1701.08230.pdf?source=post_page }}{\fldrslt{https://arxiv.org/pdf/1701.08230.pdf?source=post_page\ul0\cf0}}}}\f0\fs22\par
Cowgill, B., Dell'Acqua, F., Deng, S., Hsu, D., Verma, N., & Chaintreau, A. (2020, July). Biased programmers? or biased data? a field experiment in operationalizing ai ethics. In Proceedings of the 21st ACM Conference on Economics and Computation (pp. 679-681). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/2012.02394 }}{\fldrslt{https://arxiv.org/pdf/2012.02394\ul0\cf0}}}}\f0\fs22\par
Cronin, A. M., & Vickers, A. J. (2008). Statistical methods to correct for verification bias in diagnostic studies are inadequate when there are few false negatives: a simulation study. BMC Medical Research Methodology, 8(1), 1-9. {{\field{\*\fldinst{HYPERLINK https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-8-75 }}{\fldrslt{https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-8-75\ul0\cf0}}}}\f0\fs22\par
D'Amour, A., Srinivasan, H., Atwood, J., Baljekar, P., Sculley, D., & Halpern, Y. (2020, January). Fairness is not static: deeper understanding of long term fairness via simulation studies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (pp. 525-534). {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3351095.3372878 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3351095.3372878\ul0\cf0}}}}\f0\fs22\par
Datta, A., Sen, S., & Zick, Y. (2016, May). Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In 2016 IEEE symposium on security and privacy (SP) (pp. 598-617). IEEE.  {{\field{\*\fldinst{HYPERLINK https://www.frogheart.ca/wp-content/uploads/2016/06/CarnegieMellon_AlgorithmicTransparency.pdf }}{\fldrslt{https://www.frogheart.ca/wp-content/uploads/2016/06/CarnegieMellon_AlgorithmicTransparency.pdf\ul0\cf0}}}}\f0\fs22\par
de Greeff, J., de Boer, M. H., Hillerstr\'f6m, F. H., Bomhof, F., Jorritsma, W., & Neerincx, M. A. (2021). The FATE System: FAir, Transparent and Explainable Decision Making. In AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering. {{\field{\*\fldinst{HYPERLINK http://ceur-ws.org/Vol-2846/paper35.pdf }}{\fldrslt{http://ceur-ws.org/Vol-2846/paper35.pdf\ul0\cf0}}}}\f0\fs22\par
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference (pp. 214-226). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1104.3913 }}{\fldrslt{https://arxiv.org/pdf/1104.3913\ul0\cf0}}}}\f0\fs22\par
Eitel-Porter, R. (2021). Beyond the promise: implementing ethical AI. AI and Ethics, 1(1), 73-80. {{\field{\*\fldinst{HYPERLINK https://link.springer.com/article/10.1007/s43681-020-00011-6 }}{\fldrslt{https://link.springer.com/article/10.1007/s43681-020-00011-6\ul0\cf0}}}}\f0\fs22\par
Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 3315-3323. {{\field{\*\fldinst{HYPERLINK https://papers.nips.cc/paper/6374-interaction-screening-efficient-and-sample-optimal-learning-of-ising-models.pdf }}{\fldrslt{https://papers.nips.cc/paper/6374-interaction-screening-efficient-and-sample-optimal-learning-of-ising-models.pdf\ul0\cf0}}}}\f0\fs22\par
Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015, August). Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 259-268). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1412.3756.pdf }}{\fldrslt{https://arxiv.org/pdf/1412.3756.pdf\ul0\cf0}}}}\f0\fs22 ;\par
Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., & Roth, D. (2019, January). A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the conference on fairness, accountability, and transparency (pp. 329-338). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1802.04422 }}{\fldrslt{https://arxiv.org/pdf/1802.04422\ul0\cf0}}}}\f0\fs22\par
George, Joey F., Kevin Duffy, and Manju Ahuja. "Countering the anchoring and adjustment bias with decision support systems." Decision Support Systems 29.2 (2000): 195-206. {{\field{\*\fldinst{HYPERLINK http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.3329&rep=rep1&type=pdf }}{\fldrslt{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.3329&rep=rep1&type=pdf\ul0\cf0}}}}\f0\fs22\par
Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., & Weller, A. (2016, December). The case for process fairness in learning: Feature selection for fair decision making. In NIPS Symposium on Machine Learning and the Law (Vol. 1, p. 2). {{\field{\*\fldinst{HYPERLINK http://www.mlandthelaw.org/papers/grgic.pdf }}{\fldrslt{http://www.mlandthelaw.org/papers/grgic.pdf\ul0\cf0}}}}\f0\fs22\par
Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 3315-3323. {{\field{\*\fldinst{HYPERLINK https://papers.nips.cc/paper/6374-interaction-screening-efficient-and-sample-optimal-learning-of-ising-models.pdf }}{\fldrslt{https://papers.nips.cc/paper/6374-interaction-screening-efficient-and-sample-optimal-learning-of-ising-models.pdf\ul0\cf0}}}}\f0\fs22\par
Helwegen, R., Louizos, C., & Forr\'e9, P. (2020). Improving fair predictions using variational inference in causal models. arXiv preprint arXiv:2008.10880. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/2008.10880 }}{\fldrslt{https://arxiv.org/pdf/2008.10880\ul0\cf0}}}}\f0\fs22\par
Holstein, K., Wortman Vaughan, J., Daum\'e9 III, H., Dudik, M., & Wallach, H. (2019, May). Improving fairness in machine learning systems: What do industry practitioners need?. In Proceedings of the 2019 CHI conference on human factors in computing systems (pp. 1-16). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1812.05239 }}{\fldrslt{https://arxiv.org/pdf/1812.05239\ul0\cf0}}}}\f0\fs22\par
Hooker, S. (2021). Moving beyond \ldblquote algorithmic bias is a data problem\rdblquote . Patterns, 2(4), 100241. {{\field{\*\fldinst{HYPERLINK https://www.sciencedirect.com/science/article/pii/S2666389921000611 }}{\fldrslt{https://www.sciencedirect.com/science/article/pii/S2666389921000611\ul0\cf0}}}}\f0\fs22\par
Hutchinson, B., & Mitchell, M. (2019, January). 50 years of test (un) fairness: Lessons for machine learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 49-58). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1811.10104 }}{\fldrslt{https://arxiv.org/pdf/1811.10104\ul0\cf0}}}}\f0\fs22\par
Jacobs, A. Z., & Wallach, H. (2021, March). Measurement and fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 375-385). {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3442188.3445901 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3442188.3445901\ul0\cf0}}}}\f0\fs22\par
Kay, M., Matuszek, C., & Munson, S. A. (2015, April). Unequal representation and gender stereotypes in image search results for occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (pp. 3819-3828). {{\field{\*\fldinst{HYPERLINK https://mdsoar.org/bitstream/handle/11603/11254/KayMatuszekMunsonCHI2015GenderImageSearch.pdf?sequence=1 }}{\fldrslt{https://mdsoar.org/bitstream/handle/11603/11254/KayMatuszekMunsonCHI2015GenderImageSearch.pdf?sequence=1\ul0\cf0}}}}\f0\fs22\par
Kilbertus, N., Rojas-Carulla, M., Parascandolo, G., Hardt, M., Janzing, D., & Sch\'f6lkopf, B. (2017). Avoiding discrimination through causal reasoning. arXiv preprint arXiv:1706.02744. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1706.02744 }}{\fldrslt{https://arxiv.org/pdf/1706.02744\ul0\cf0}}}}\f0\fs22\par
Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1609.05807 }}{\fldrslt{https://arxiv.org/pdf/1609.05807\ul0\cf0}}}}\f0\fs22\par
Kusner, M. J., Loftus, J. R., Russell, C., & Silva, R. (2017). Counterfactual fairness. arXiv preprint arXiv:1703.06856. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1703.06856 }}{\fldrslt{https://arxiv.org/pdf/1703.06856\ul0\cf0}}}}\f0\fs22\par
Lee, M. K. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big Data & Society, 5(1), 2053951718756684. {{\field{\*\fldinst{HYPERLINK https://journals.sagepub.com/doi/pdf/10.1177/2053951718756684 }}{\fldrslt{https://journals.sagepub.com/doi/pdf/10.1177/2053951718756684\ul0\cf0}}}}\f0\fs22\par
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6), 1-35. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1908.09635 }}{\fldrslt{https://arxiv.org/pdf/1908.09635\ul0\cf0}}}}\f0\fs22\par
Olteanu, A., Castillo, C., Diaz, F., & K\f1\u305?c\u305?man, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13.\f0\lang1033  {{\field{\*\fldinst{HYPERLINK https://www.frontiersin.org/articles/10.3389/fdata.2019.00013/full }}{\fldrslt{https://www.frontiersin.org/articles/10.3389/fdata.2019.00013/full\ul0\cf0}}}}\f0\fs22\par
Pessach, D., & Shmueli, E. (2020). Algorithmic fairness. arXiv preprint arXiv:2001.09784. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/2001.09784 }}{\fldrslt{https://arxiv.org/pdf/2001.09784\ul0\cf0}}}}\f0\fs22\par
Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., & Denton, E. (2020, February). Saving face: Investigating the ethical concerns of facial recognition auditing. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 145-151). {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3375627.3375820 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3375627.3375820\ul0\cf0}}}}\f0\fs22\par
Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019, January). Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency (pp. 59-68). {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3287560.3287598 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3287560.3287598\ul0\cf0}}}}\f0\fs22\par
Sweeney, L. (2013). Discrimination in online ad delivery. Communications of the ACM, 56(5), 44-54. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1301.6822 }}{\fldrslt{https://arxiv.org/pdf/1301.6822\ul0\cf0}}}}\f0\fs22\par
Torralba, A., & Efros, A. A. (2011, June). Unbiased look at dataset bias. In CVPR 2011 (pp. 1521-1528). IEEE. {{\field{\*\fldinst{HYPERLINK http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.944.9518&rep=rep1&type=pdf }}{\fldrslt{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.944.9518&rep=rep1&type=pdf\ul0\cf0}}}}\f0\fs22\par
Verma, S., & Rubin, J. (2018, May). Fairness definitions explained. In 2018 ieee/acm international workshop on software fairness (fairware) (pp. 1-7). IEEE. {{\field{\*\fldinst{HYPERLINK https://fairware.cs.umass.edu/papers/Verma.pdf }}{\fldrslt{https://fairware.cs.umass.edu/papers/Verma.pdf\ul0\cf0}}}}\f0\fs22  \par
Yang, K., Huang, B., Stoyanovich, J., & Schelter, S. (2020, January). Fairness-Aware Instrumentation of Preprocessing~ Pipelines for Machine Learning. In Workshop on Human-In-the-Loop Data Analytics (HILDA'20). {{\field{\*\fldinst{HYPERLINK https://par.nsf.gov/servlets/purl/10182459 }}{\fldrslt{https://par.nsf.gov/servlets/purl/10182459\ul0\cf0}}}}\f0\fs22\par
Zemel, R., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013, May). Learning fair representations. In International conference on machine learning (pp. 325-333). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v28/zemel13.pdf }}{\fldrslt{http://proceedings.mlr.press/v28/zemel13.pdf\ul0\cf0}}}}\f0\fs22\par
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2017). Men also like shopping: Reducing gender bias amplification using corpus-level constraints. arXiv preprint arXiv:1707.09457. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1707.09457 }}{\fldrslt{https://arxiv.org/pdf/1707.09457\ul0\cf0}}}}\f0\fs22\par
\lang9\par
}
 