{\rtf1\ansi\ansicpg1252\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE access, 6, 52138-52160. {{\field{\*\fldinst{HYPERLINK https://ieeexplore.ieee.org/iel7/6287639/6514899/08466590.pdf }}{\fldrslt{https://ieeexplore.ieee.org/iel7/6287639/6514899/08466590.pdf\ul0\cf0}}}}\f0\fs22\par
Akata, Z., Balliet, D., De Rijke, M., Dignum, F., Dignum, V., Eiben, G., ... & Welling, M. (2020). A research agenda for hybrid intelligence: augmenting human intellect with collaborative, adaptive, responsible, and explainable artificial intelligence. IEEE Annals of the History of Computing, 53(08), 18-28. {{\field{\*\fldinst{HYPERLINK https://vossen.info/wp-content/uploads/2020/08/akata-2020-research.pdf }}{\fldrslt{https://vossen.info/wp-content/uploads/2020/08/akata-2020-research.pdf\ul0\cf0}}}}\f0\fs22\par
Anderson, A., Dodge, J., Sadarangani, A., Juozapaitis, Z., Newman, E., Irvine, J., ... & Burnett, M. (2020). Mental models of mere mortals with explanations of reinforcement learning. ACM Transactions on Interactive Intelligent Systems (TiiS), 10(2), 1-37. {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3366485 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3366485\ul0\cf0}}}}\f0\fs22\par
Anjomshoae, S., Najjar, A., Calvaresi, D., & Fr\'e4mling, K. (2019). Explainable agents and robots: Results from a systematic literature review. In 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\f1\endash 17, 2019 (pp. 1078-1088). International Foundation for Autonomous Agents and Multiagent Systems.\f0\lang1033  {{\field{\*\fldinst{HYPERLINK https://www.diva-portal.org/smash/get/diva2:1303810/FULLTEXT01.pdf }}{\fldrslt{https://www.diva-portal.org/smash/get/diva2:1303810/FULLTEXT01.pdf\ul0\cf0}}}}\f0\fs22\par
Barocas, S., Selbst, A. D., & Raghavan, M. (2020, January). The hidden assumptions behind counterfactual explanations and principal reasons. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (pp. 80-89). {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3351095.3372830 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3351095.3372830\ul0\cf0}}}}\f0\fs22\par
Arrieta, A. B., D\'edaz-Rodr\'edguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82-115. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1910.10045 }}{\fldrslt{https://arxiv.org/pdf/1910.10045\ul0\cf0}}}}\f0\fs22\par
Broekens, J., Harbers, M., Hindriks, K., Van Den Bosch, K., Jonker, C., & Meyer, J. J. (2010, September). Do you get it? User-evaluated explainable BDI agents. In German Conference on Multiagent System Technologies (pp. 28-39). Springer, Berlin, Heidelberg. {{\field{\*\fldinst{HYPERLINK https://www.researchgate.net/profile/Maaike_Harbers/publication/221248771_Do_you_get_it_User-evaluated_explainable_BDI_agents/links/02e7e51a3698beeec1000000/Do-you-get-it-User-evaluated-explainable-BDI-agents.pdf }}{\fldrslt{https://www.researchgate.net/profile/Maaike_Harbers/publication/221248771_Do_you_get_it_User-evaluated_explainable_BDI_agents/links/02e7e51a3698beeec1000000/Do-you-get-it-User-evaluated-explainable-BDI-agents.pdf\ul0\cf0}}}}\f0\fs22\par
Byrne, R. M. (2019, August). Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning. In IJCAI (pp. 6276-6282). {{\field{\*\fldinst{HYPERLINK https://www.ijcai.org/proceedings/2019/0876.pdf }}{\fldrslt{https://www.ijcai.org/proceedings/2019/0876.pdf\ul0\cf0}}}}\f0\fs22\par
Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1702.08608 }}{\fldrslt{https://arxiv.org/pdf/1702.08608\ul0\cf0}}}}\f0\fs22\par
Fox, M., Long, D., & Magazzeni, D. (2017). Explainable planning. arXiv preprint arXiv:1709.10256. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1709.10256 }}{\fldrslt{https://arxiv.org/pdf/1709.10256\ul0\cf0}}}}\f0\fs22\par
Gomez, O., Holter, S., Yuan, J., & Bertini, E. (2020, March). ViCE: visual counterfactual explanations for machine learning models. In Proceedings of the 25th International Conference on Intelligent User Interfaces (pp. 531-535). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/2003.02428 }}{\fldrslt{https://arxiv.org/pdf/2003.02428\ul0\cf0}}}}\f0\fs22\par
Gregor, S., & Benbasat, I. (1999). Explanations from intelligent systems: Theoretical foundations and implications for practice. MIS quarterly, 497-530. {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/abs/10.2307/249487 }}{\fldrslt{https://dl.acm.org/doi/abs/10.2307/249487\ul0\cf0}}}}\f0\fs22\par
Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5), 1-42. {{\field{\*\fldinst{HYPERLINK https://dl.acm.org/doi/pdf/10.1145/3236009 }}{\fldrslt{https://dl.acm.org/doi/pdf/10.1145/3236009\ul0\cf0}}}}\f0\fs22\par
Harbers, M., van den Bosch, K., & Meyer, J. J. (2010, August). Design and evaluation of explainable BDI agents. In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (Vol. 2, pp. 125-132). IEEE. {{\field{\*\fldinst{HYPERLINK https://www.karelvandenbosch.nl/documents/2010_Harbers_etal_IAT_Design_and_Evaluation_of_Explainable_BDI_Agents.pdf }}{\fldrslt{https://www.karelvandenbosch.nl/documents/2010_Harbers_etal_IAT_Design_and_Evaluation_of_Explainable_BDI_Agents.pdf\ul0\cf0}}}}\f0\fs22\par
Hayes, B., & Shah, J. A. (2017, March). Improving robot controller transparency through autonomous policy explanation. In 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI (pp. 303-312). IEEE. {{\field{\*\fldinst{HYPERLINK https://dspace.mit.edu/bitstream/handle/1721.1/116013/hri17.pdf?sequence=1&isAllowed=y }}{\fldrslt{https://dspace.mit.edu/bitstream/handle/1721.1/116013/hri17.pdf?sequence=1&isAllowed=y\ul0\cf0}}}}\f0\fs22\par
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & M\'fcller, H. (2019). Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), e1312. {{\field{\*\fldinst{HYPERLINK https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1312 }}{\fldrslt{https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1312\ul0\cf0}}}}\f0\fs22\par
Janzing, D., Minorics, L., & Bl\'f6baum, P. (2020, June). Feature relevance quantification in explainable AI: A causal problem. In International Conference on Artificial Intelligence and Statistics (pp. 2907-2916). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf }}{\fldrslt{http://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf\ul0\cf0}}}}\f0\fs22\par
Kaptein, F., Broekens, J., Hindriks, K., & Neerincx, M. (2017, August). Personalised self-explanation by robots: The role of goals versus beliefs in robot-action explanation for children and adults. In 2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN) (pp. 676-682). IEEE. {{\field{\*\fldinst{HYPERLINK https://www.researchgate.net/profile/Frank-Kaptein/publication/321811208_Personalised_self-explanation_by_robots_The_role_of_goals_versus_beliefs_in_robot-action_explanation_for_children_and_adults/links/5a38de3f0f7e9b7c4870083e/Personalised-self-explanation-by-robots-The-role-of-goals-versus-beliefs-in-robot-action-explanation-for-children-and-adults.pdf }}{\fldrslt{https://www.researchgate.net/profile/Frank-Kaptein/publication/321811208_Personalised_self-explanation_by_robots_The_role_of_goals_versus_beliefs_in_robot-action_explanation_for_children_and_adults/links/5a38de3f0f7e9b7c4870083e/Personalised-self-explanation-by-robots-The-role-of-goals-versus-beliefs-in-robot-action-explanation-for-children-and-adults.pdf\ul0\cf0}}}}\f0\fs22\par
Kaur, H., Nori, H., Jenkins, S., Caruana, R., Wallach, H., & Wortman Vaughan, J. (2020, April). Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-14). {{\field{\*\fldinst{HYPERLINK http://www.jennwv.com/papers/interp-ds.pdf }}{\fldrslt{http://www.jennwv.com/papers/interp-ds.pdf\ul0\cf0}}}}\f0\fs22\par
Koh, P. W., & Liang, P. (2017, July). Understanding black-box predictions via influence functions. In International Conference on Machine Learning (pp. 1885-1894). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v70/koh17a/koh17a.pdf }}{\fldrslt{http://proceedings.mlr.press/v70/koh17a/koh17a.pdf\ul0\cf0}}}}\f0\fs22\par
Kumar, I. E., Venkatasubramanian, S., Scheidegger, C., & Friedler, S. (2020, November). Problems with Shapley-value-based explanations as feature importance measures. In International Conference on Machine Learning (pp. 5491-5500). PMLR. {{\field{\*\fldinst{HYPERLINK http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf }}{\fldrslt{http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf\ul0\cf0}}}}\f0\fs22\par
Langley, P., Meadows, B., Sridharan, M., & Choi, D. (2017, February). Explainable agency for intelligent autonomous systems. In Twenty-Ninth IAAI Conference. {{\field{\*\fldinst{HYPERLINK https://www.aaai.org/ocs/index.php/IAAI/IAAI17/paper/viewPDFInterstitial/15046/13734 }}{\fldrslt{https://www.aaai.org/ocs/index.php/IAAI/IAAI17/paper/viewPDFInterstitial/15046/13734\ul0\cf0}}}}\f0\fs22\par
Lecue, F. (2020). On the role of knowledge graphs in explainable AI. Semantic Web, 11(1), 41-51. {{\field{\*\fldinst{HYPERLINK http://semantic-web-journal.org/system/files/swj2259.pdf }}{\fldrslt{http://semantic-web-journal.org/system/files/swj2259.pdf\ul0\cf0}}}}\f0\fs22\par
\lang9 Letham, B., Rudin, C., McCormick, T. H., & Madigan, D. (2015). Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3), 1350-1371. {{\field{\*\fldinst{HYPERLINK https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/Interpretable-classifiers-using-rules-and-Bayesian-analysis--Building-a/10.1214/15-AOAS848.pdf }}{\fldrslt{https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/Interpretable-classifiers-using-rules-and-Bayesian-analysis--Building-a/10.1214/15-AOAS848.pdf\ul0\cf0}}}}\f0\fs22\par
Lim, B. Y., Dey, A. K., & Avrahami, D. (2009, April). Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 2119-2128). {{\field{\*\fldinst{HYPERLINK https://www.academia.edu/download/6079173/lim_chi_09.pdf }}{\fldrslt{https://www.academia.edu/download/6079173/lim_chi_09.pdf\ul0\cf0}}}}\f0\fs22\par
Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., ... & Lee, S. I. (2020). From local explanations to global understanding with explainable AI for trees. Nature machine intelligence, 2(1), 56-67. {{\field{\*\fldinst{HYPERLINK https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7326367/ }}{\fldrslt{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7326367/\ul0\cf0}}}}\f0\fs22\par
Lyons, J. B., & Havig, P. R. (2014, June). Transparency in a human-machine context: approaches for fostering shared awareness/intent. In International conference on virtual, augmented and mixed reality (pp. 181-190). Springer, Cham. {{\field{\*\fldinst{HYPERLINK https://link.springer.com/content/pdf/10.1007/978-3-319-07458-0_18.pdf }}{\fldrslt{https://link.springer.com/content/pdf/10.1007/978-3-319-07458-0_18.pdf\ul0\cf0}}}}\f0\fs22\par
Madumal, P., Miller, T., Sonenberg, L., & Vetere, F. (2020, April). Explainable reinforcement learning through a causal lens. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 03, pp. 2493-2500). {{\field{\*\fldinst{HYPERLINK https://ojs.aaai.org/index.php/AAAI/article/view/5631/5487 }}{\fldrslt{https://ojs.aaai.org/index.php/AAAI/article/view/5631/5487\ul0\cf0}}}}\f0\fs22\par
Miller, T. (2018). Contrastive explanation: A structural-model approach. arXiv preprint arXiv:1811.03163. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1811.03163 }}{\fldrslt{https://arxiv.org/pdf/1811.03163\ul0\cf0}}}}\f0\fs22\par
Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267, 1-38. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1706.07269 }}{\fldrslt{https://arxiv.org/pdf/1706.07269\ul0\cf0}}}}\f0\fs22\par
Miller, T., Howe, P., & Sonenberg, L. (2017). Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences. arXiv preprint arXiv:1712.00547. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1712.00547 }}{\fldrslt{https://arxiv.org/pdf/1712.00547\ul0\cf0}}}}\f0\fs22\par
Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... & Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1810.03993.pdf?source=post_page }}{\fldrslt{https://arxiv.org/pdf/1810.03993.pdf?source=post_page\ul0\cf0}}}}\f0\fs22\par
Mittelstadt, B., Russell, C., & Wachter, S. (2019, January). Explaining explanations in AI. In Proceedings of the conference on fairness, accountability, and transparency (pp. 279-288). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1811.01439 }}{\fldrslt{https://arxiv.org/pdf/1811.01439\ul0\cf0}}}}\f0\fs22\par
Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). Interpretml: A unified framework for machine learning interpretability. arXiv preprint arXiv:1909.09223. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1909.09223 }}{\fldrslt{https://arxiv.org/pdf/1909.09223\ul0\cf0}}}}\f0\fs22\par
Poyiadzi, R., Sokol, K., Santos-Rodriguez, R., De Bie, T., & Flach, P. (2020, February). FACE: Feasible and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 344-350). {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1909.09369 }}{\fldrslt{https://arxiv.org/pdf/1909.09369\ul0\cf0}}}}\f0\fs22\par
Preece, A. (2018). Asking \lquote Why\rquote in AI: Explainability of intelligent systems\f1\endash perspectives and challenges. Intelligent Systems in Accounting, Finance and Management, 25(2), 63-72.\f0\lang1033  {{\field{\*\fldinst{HYPERLINK https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/isaf.1422 }}{\fldrslt{https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/isaf.1422\ul0\cf0}}}}\f0\fs22\par
Rathi, S. (2019). Generating counterfactual and contrastive explanations using SHAP. arXiv preprint arXiv:1906.09293. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1906.09293 }}{\fldrslt{https://arxiv.org/pdf/1906.09293\ul0\cf0}}}}\f0\fs22\par
Ribeiro, M. T., Singh, S., & Guestrin, C. (2018, April). Anchors: High-precision model-agnostic explanations. In Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1). {{\field{\*\fldinst{HYPERLINK https://ojs.aaai.org/index.php/AAAI/article/view/11491/11350 }}{\fldrslt{https://ojs.aaai.org/index.php/AAAI/article/view/11491/11350\ul0\cf0}}}}\f0\fs22\par
Roscher, R., Bohn, B., Duarte, M. F., & Garcke, J. (2020). Explainable machine learning for scientific insights and discoveries. Ieee Access, 8, 42200-42216. {{\field{\*\fldinst{HYPERLINK https://ieeexplore.ieee.org/iel7/6287639/8948470/09007737.pdf }}{\fldrslt{https://ieeexplore.ieee.org/iel7/6287639/8948470/09007737.pdf\ul0\cf0}}}}\f0\fs22\par
Samek, W., & M\'fcller, K. R. (2019). Towards explainable artificial intelligence. In Explainable AI: interpreting, explaining and visualizing deep learning (pp. 5-22). Springer, Cham. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1909.12072 }}{\fldrslt{https://arxiv.org/pdf/1909.12072\ul0\cf0}}}}\f0\fs22\par
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision (pp. 618-626). {{\field{\*\fldinst{HYPERLINK http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf }}{\fldrslt{http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf\ul0\cf0}}}}\f0\fs22\par
Stepin, I., Alonso, J. M., Catala, A., & Pereira-Fari\'f1a, M. (2021). A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence. IEEE Access, 9, 11974-12001. {{\field{\*\fldinst{HYPERLINK https://ieeexplore.ieee.org/iel7/6287639/9312710/09321372.pdf }}{\fldrslt{https://ieeexplore.ieee.org/iel7/6287639/9312710/09321372.pdf\ul0\cf0}}}}\f0\fs22\par
van der Waa, J., van Diggelen, J., & Neerincx, M. (2018). The design and validation of an intuitive confidence measure. memory, 2, 1. {{\field{\*\fldinst{HYPERLINK https://www.researchgate.net/profile/Jasper_Waa/publication/323772259_The_design_and_validation_of_an_intuitive_confidence_measure/links/5aaa2ba0aca272d39cd64796/The-design-and-validation-of-an-intuitive-confidence-measure.pdf }}{\fldrslt{https://www.researchgate.net/profile/Jasper_Waa/publication/323772259_The_design_and_validation_of_an_intuitive_confidence_measure/links/5aaa2ba0aca272d39cd64796/The-design-and-validation-of-an-intuitive-confidence-measure.pdf\ul0\cf0}}}}\f0\fs22\par
Tintarev, N., & Masthoff, J. (2007, October). Effective explanations of recommendations: user-centered design. In Proceedings of the 2007 ACM conference on Recommender systems (pp. 153-156). {{\field{\*\fldinst{HYPERLINK https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.888.3437&rep=rep1&type=pdf }}{\fldrslt{https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.888.3437&rep=rep1&type=pdf\ul0\cf0}}}}\f0\fs22\par
van der Waa, J., van Diggelen, J., Bosch, K. V. D., & Neerincx, M. (2018). Contrastive explanations for reinforcement learning in terms of expected consequences. arXiv preprint arXiv:1807.08706. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1807.08706 }}{\fldrslt{https://arxiv.org/pdf/1807.08706\ul0\cf0}}}}\f0\fs22\par
van der Waa, J., Robeer, M., van Diggelen, J., Brinkhuis, M., & Neerincx, M. (2018). Contrastive explanations with local foil trees. arXiv preprint arXiv:1806.07470. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1806.07470.pdf&sa=D&ust=1537878080177000 }}{\fldrslt{https://arxiv.org/pdf/1806.07470.pdf&sa=D&ust=1537878080177000\ul0\cf0}}}}\f0\fs22\par
van der Waa, J., Nieuwburg, E., Cremers, A., & Neerincx, M. (2021). Evaluating XAI: A comparison of rule-based and example-based explanations. Artificial Intelligence, 291, 103404. {{\field{\*\fldinst{HYPERLINK https://www.sciencedirect.com/science/article/pii/S0004370220301533 }}{\fldrslt{https://www.sciencedirect.com/science/article/pii/S0004370220301533\ul0\cf0}}}}\f0\fs22\par
van der Waa, J., Schoonderwoerd, T., van Diggelen, J., & Neerincx, M. (2020). Interpretable confidence measures for decision support systems. International Journal of Human-Computer Studies, 144, 102493. {{\field{\*\fldinst{HYPERLINK https://www.sciencedirect.com/science/article/pii/S1071581920300951 }}{\fldrslt{https://www.sciencedirect.com/science/article/pii/S1071581920300951\ul0\cf0}}}}\f0\fs22\par
Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Transparent, explainable, and accountable AI for robotics. Science (Robotics), 2(6). {{\field{\*\fldinst{HYPERLINK https://philarchive.org/archive/WACTEA }}{\fldrslt{https://philarchive.org/archive/WACTEA\ul0\cf0}}}}\f0\fs22\par
Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841. {{\field{\*\fldinst{HYPERLINK https://arxiv.org/pdf/1711.00399.pdf?source=post_page }}{\fldrslt{https://arxiv.org/pdf/1711.00399.pdf?source=post_page\ul0\cf0}}}}\f0\fs22\par
Wang, N., Pynadath, D. V., & Hill, S. G. (2016, March). Trust calibration within a human-robot team: Comparing automatically generated explanations. In 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI) (pp. 109-116). IEEE. {{\field{\*\fldinst{HYPERLINK https://www.cs.utexas.edu/users/sniekum/classes/RLFD-F16/papers/Trust.pdf }}{\fldrslt{https://www.cs.utexas.edu/users/sniekum/classes/RLFD-F16/papers/Trust.pdf\ul0\cf0}}}}\f0\fs22\par
Wang, D., Yang, Q., Abdul, A., & Lim, B. Y. (2019, May). Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems (pp. 1-15). {{\field{\*\fldinst{HYPERLINK http://www.brianlim.net/wordpress/wp-content/uploads/2019/01/chi2019-reasoned-xai-framework.pdf }}{\fldrslt{http://www.brianlim.net/wordpress/wp-content/uploads/2019/01/chi2019-reasoned-xai-framework.pdf\ul0\cf0}}}}\f0\fs22\par
Weitz, K., Schiller, D., Schlagowski, R., Huber, T., & Andr\'e9, E. (2019, July). " Do you trust me?" Increasing user-trust by integrating virtual agents in explainable AI interaction design. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 7-9). {{\field{\*\fldinst{HYPERLINK https://opus.bibliothek.uni-augsburg.de/opus4/files/65191/X_Plane_IVA_2019_EA.pdf }}{\fldrslt{https://opus.bibliothek.uni-augsburg.de/opus4/files/65191/X_Plane_IVA_2019_EA.pdf\ul0\cf0}}}}\f0\fs22\par
\lang9 Yang, X. J., Unhelkar, V. V., Li, K., & Shah, J. A. (2017, March). Evaluating effects of user experience and system transparency on trust in automation. In 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI (pp. 408-416). IEEE. {{\field{\*\fldinst{HYPERLINK https://dspace.mit.edu/bitstream/handle/1721.1/116045/Yang_HRI_2017.pdf?sequence=1&isAllowed=y }}{\fldrslt{https://dspace.mit.edu/bitstream/handle/1721.1/116045/Yang_HRI_2017.pdf?sequence=1&isAllowed=y\ul0\cf0}}}}\f0\fs22\par
\par
}
 